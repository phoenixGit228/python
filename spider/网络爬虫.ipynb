{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 爬虫大纲\n",
    "\n",
    "- `第0关`，初识爬虫，\n",
    "\n",
    "  了解爬虫的工作原理，写出一个简单的爬虫程序，学会爬虫的第0步：获取数据。\n",
    "\n",
    "- `第1关`，你将会快速入门HTML基础知识\n",
    "\n",
    "  达到读懂和修改HTML文档的水平。有了这些基础，你才能去学习如何解析数据和提取数据。\n",
    "\n",
    "- `第2、3、4、5关`，我会教你爬虫的第1和第2步：解析数据和提取数据。\n",
    "  同时，你还会学到两种不同的发起请求的方式。\n",
    "\n",
    "- `第6关`，你将学会存储数据，即把目标数据写入到本地的Excel表格中。\n",
    "  到此，你就学会了爬虫完整的四个步骤，掌握了最基本的爬虫技能啦。\n",
    "\n",
    "- `第7关`，我们一起做一个项目，爬取一个知乎大v的所有文章，并且存到Excel中。\n",
    "  以此，我们巩固和复习了0-6关的所有知识。第7关会是一个分水岭，后面关卡的进阶知识都建立在前7关的基础上。\n",
    "\n",
    "- `第8关`，学会cookies，\n",
    "  可以让浏览器记住你，你们可以更方便地长期保持联系，而不是在一次见面之后就相忘于江湖。\n",
    "\n",
    "- `第9关`，学习控制浏览器，来应对爬虫中一些更复杂的情况。\n",
    "\n",
    "- `第10关`，你的爬虫会变得更自动化，爬虫程序不但可以定时工作，还可以把爬取结果传递给你。\n",
    "\n",
    "- `接下来的4关`，你将学会更高效更强大的爬虫方法，让爬虫技能升级。\n",
    "\n",
    "- `第15关`,毕业总结，就到了告别的时刻了。\n",
    "  这时你也学成出师，可以用爬虫知识去做自己想做的事情了，让爬虫为你消灭重复劳动，高效获取信息，创造出更多价值。\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 第0关 初识爬虫\n",
    "\n",
    "## 方法\n",
    "import requests\n",
    "res = requests.get('URL')\n",
    "\n",
    "## 请求结果属性\n",
    "- res.content\n",
    "- res.text\n",
    "- res.encoding\n",
    "- res.status.code\n",
    "\n",
    "\n",
    "## 爬虫四个步骤\n",
    "- 获取数据\n",
    "- 提取数据\n",
    "- 解析数据\n",
    "- 储存数据\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 引入requests库\r\n",
    "import requests\r\n",
    "\r\n",
    "# requests.get是在调用requests库中的get()方法，它向服务器发送了一个请求，括号里的参数是你需要的数据所在的网址，然后服务器对请求作出了响应。\r\n",
    "# 我们把这个响应返回的结果赋值给变量res\r\n",
    "res = requests.get('https://localprod.pandateacher.com/python-manuscript/crawler-html/sanguo.md')\r\n",
    "\r\n",
    "# res类型\r\n",
    "print(type(res))\r\n",
    "\r\n",
    "# 请求状态码\r\n",
    "print(res.status_code)\r\n",
    "\r\n",
    "# 获取编码格式，requests如果判断编码格式不正确，需要使用encoding属性解决\r\n",
    "res.encoding\r\n",
    "\r\n",
    "# 获取结果的二进制，可用于图片，音频，视频\r\n",
    "res.content\r\n",
    "\r\n",
    "# 获取结果的字符串格式，适用于文字、网页源代码的下载\r\n",
    "txt = res.text\r\n",
    "txt[:100]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import requests\r\n",
    "res = requests.get(\"https://res.pandateacher.com/2018-12-18-10-43-07.png\")\r\n",
    "pic = res.content\r\n",
    "with open(\"ppt.png\", \"wb\") as file1:\r\n",
    "    file1.write(pic)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 引入requests库\r\n",
    "import requests\r\n",
    "#下载《三国演义》第一回，我们得到一个对象，它被命名为res\r\n",
    "res = requests.get('https://localprod.pandateacher.com/python-manuscript/crawler-html/sanguo.md')\r\n",
    "# 把Response对象的内容以字符串的形式返回\r\n",
    "novel = res.text\r\n",
    "code = res.encoding\r\n",
    "print(code)\r\n",
    "# 创建一个名为《三国演义》的txt文档，指针放在文件末尾，追加内容\r\n",
    "k = open('《三国演义》.md','a+',encoding=res.encoding)\r\n",
    "# k = open('《三国演义》.md','a+')\r\n",
    "# 写进文件中 \r\n",
    "k.write(novel)\r\n",
    "# 关闭文档    \r\n",
    "k.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 爬虫道德\n",
    "\n",
    "道德规范都在**`robots.txt`**协议里。该协议是互联网爬虫的一项公认的道德规范，它的全称是“网络爬虫排除标准”（robots exclusion protocol），这个协议用来告诉爬虫，哪些页面是可以抓取的，哪些不可以。\n",
    "\n",
    "\n",
    "User-Agent: *\n",
    "Allow: /\n",
    "Disallow: /"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 第1关：我也可以写一个网页\n",
    "\n",
    "## HTML组成\n",
    "\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "\n",
    "<html>\n",
    "    <title>\n",
    "        标题标签\n",
    "    </title>\n",
    "\n",
    "    <head>\n",
    "        网页头的具体内容\n",
    "        <h1> <h2> <h3> 定义标题\n",
    "    </head>\n",
    "    <body>\n",
    "        网页体的具体内容\n",
    "        <p> </p>  定义段落\n",
    "        <div>\n",
    "            块标签\n",
    "        </div>\n",
    "\n",
    "        <img>  图像标签\n",
    "        \n",
    "        <form>\n",
    "            表单标签\n",
    "        </form>\n",
    "    </body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "## 标签\n",
    "### 闭合标签\n",
    "\n",
    "`<> </>` 成对出现\n",
    "\n",
    "### 空标签\n",
    "\n",
    "只有一个尖括号`<>`（斜杠/可省略），标签开始即结束，比如上面的`<img />`是图片标签，`<link />` 是链接标签，`<input />`是input标签。\n",
    "\n",
    "## 属性\n",
    "    \n",
    "- 标签内的赋值语句 `(width = 200px)`\n",
    "- 元素——开始标签和结束标签内的所有元素\n",
    "- `class`可以定义同种类型的元素，定义类型用`.`，定义id使用`#`\n",
    "- `style` 定义元素的行内样式\n",
    "- `id` 是定义元素的`唯一`标示\n",
    "- `herf`  定义链接\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 第2关 爬虫初体验\n",
    "```python\n",
    "import requests\n",
    "form bs4 import BeautifulSoup\n",
    "res = requests.get(URL)\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 提取数据\n",
    "- `find(), find_all()`\n",
    "- Tag对象\n",
    "\n",
    "## find用法：\n",
    "- find()   提取首个满足条件，返回；\n",
    "BeautifulSoup对象，find(标签， 属性)\n",
    "soup.find('div',class_='books')\n",
    "\n",
    "- find_all()   提取所有满足条件，返回\n",
    "BeautifulSoup对象，find_all(标签， 属性)\n",
    "soup.find_all('div',class_='books')\n",
    "\n",
    "- 括号中的`属性`和`参数`可以任选其一，也可都用\n",
    "\n",
    "## Tag用法\n",
    "- Tag.find(), Tag.find_all(), Tag中查找Tag\n",
    "- Tag.text  提取Tag中的文字\n",
    "- Tag['属性名']  提取Tag中的值\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "学习BeautifulSoup模块\n",
    "\"\"\"\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "res = requests.get('https://localprod.pandateacher.com/python-manuscript/crawler-html/spider-men5.0.html')\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "items = soup.find_all(class_='books')\n",
    "for item in items:\n",
    "    kind = item.find('h2')\n",
    "    title = item.find(class_='title')\n",
    "    info = item.find(class_='info')\n",
    "    # print(kind, '\\n', titlse, '\\n', info, '\\n')\n",
    "    print(kind.text, '\\n', title.text,'\\n',title['href'], '\\n',info.text,'\\n')\n",
    "    # print(type(item))\n",
    "    # print(item)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 第3关\n",
    "\n",
    "确认目标-分析过程-代码实现，是我们做每一个项目的必经之路。未来在此基础上，还会有许多演化，但基础都是这些。\n",
    "\n",
    "\n",
    "`将想要的数据分别提取，再做组合是一种不错的思路。但是，如果数据的数量对不上，就会让事情比较棘手。`比如，在我们的案例里，如果一个菜有多个做法，其数量也没规律，那么菜名和URL的数量就会对不上。\n",
    "\n",
    "\n",
    "`寻找最小共同父级标签是一种很常见的提取数据思路，它能有效规避这个问题。`但有时候，可能需要你反复操作，提取数据。\n",
    "\n",
    "\n",
    "所以在实际项目实操中，`需要根据情况，灵活选择，灵活组合`。我们本关卡所做的项目，只是刚刚好两种方式都可以爬取。\n",
    "\n",
    "`text`获取到的是该标签内的纯文本信息，即便是在它的子标签内，也能拿得到。但`提取属性的值，只能提取该标签本身的`。\n",
    "两种方法：\n",
    "\n",
    "- `数量太多而无规律，我们会换个标签提取；`\n",
    "- `数量不多而有规律，我们会对提取的结果进行筛选`\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# python爬虫设置请求消息头(headers)\n",
    "\n",
    "## 简述\n",
    "在使用python爬虫爬取数据的时候，经常会遇到一些网站的反爬虫措施，一般就是针对于headers中的User-Agent，如果没有对headers进行设置，User-Agent会声明自己是python脚本,而如果网站有反爬虫的想法的话，必然会拒绝这样的连接。而修改headers可以将自己的爬虫脚本伪装成浏览器的正常访问，来避免这一问题。\n",
    "\n",
    "## 设置方法\n",
    "\n",
    "### 使用requests请求页面时\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    'Accept': '*/*',\n",
    "    'Accept-Language': 'en-US,en;q=0.8',\n",
    "    'Cache-Control': 'max-age=0',\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Referer': 'http://www.baidu.com/'\n",
    "}\n",
    "resp = requests.get('http://httpbin.org/headers', headers=headers)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 使用urllib请求页面时"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import urllib, urllib2\n",
    "def get_page_source(url):\n",
    "    headers = {'Accept': '*/*',\n",
    "               'Accept-Language': 'en-US,en;q=0.8',\n",
    "               'Cache-Control': 'max-age=0',\n",
    "               'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36',\n",
    "               'Connection': 'keep-alive',\n",
    "               'Referer': 'http://www.baidu.com/'\n",
    "               }\n",
    "    req = urllib2.Request(url, None, headers)\n",
    "    response = urllib2.urlopen(req)\n",
    "    page_source = response.read()\n",
    "    return page_source"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 使用phantomjs请求页面"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "def get_headers_driver():\n",
    "    desire = DesiredCapabilities.PHANTOMJS.copy()\n",
    "    headers = {'Accept': '*/*',\n",
    "               'Accept-Language': 'en-US,en;q=0.8',\n",
    "               'Cache-Control': 'max-age=0',\n",
    "               'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36',\n",
    "               'Connection': 'keep-alive',\n",
    "               'Referer': 'http://www.baidu.com/'\n",
    "               }\n",
    "    for key, value in headers.iteritems():\n",
    "        desire['phantomjs.page.customHeaders.{}'.format(key)] = value\n",
    "    driver = webdriver.PhantomJS(desired_capabilities=desire, service_args=['--load-images=yes'])#将yes改成no可以让浏览器不加载图片\n",
    "    return driver"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 第4关 \n",
    "`Network`能够记录浏览器的所有请求。我们最常用的是：`ALL`（查看全部）/`XHR`（仅查看XHR）/Doc（Document，第0个请求一般在这里），有时候也会看看：`Img`（仅查看图片）/`Media`（仅查看媒体文件）/`Other`（其他）。最后，`JS和CSS`，则是前端代码，负责发起请求和页面实现；`Font`是文字的字体；而理解WS和Manifest，需要网络编程的知识，倘若不是专门做这个，你不需要了解。\n",
    "\n",
    "在Network，有非常重要的一类请求是`XHR`（或`Fetch`），因为有它的存在，人们不必刷新/跳转网页，即可加载新的内容。随着技术发展，XHR的应用频率越来越高，我们常常需要在这里找我们想要的数据\n",
    "\n",
    "`XHR`的功能是传输数据，其中有非常重要的一种数据是用`json`格式写成的，和html一样，这种数据能够有组织地存储大量内容。`json`的数据类型是“文本”，在Python语言当中，我们把它称为字符串。我们能够非常轻易地将json格式的数据转化为列表/字典，也能将列表/字典转为json格式的数据。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import requests\n",
    "res = requests.get(url,headers=headers)\n",
    "res.json"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "想在Python语言中，实现列表/字典转json，json转列表/字典，则需要借助json模块"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "\n",
    "# 引入json模块\n",
    "import json\n",
    "# 创建一个列表a\n",
    "a = [1,2,3,4]\n",
    "# 使用dumps()函数，将列表a转换为json格式的字符串，赋值给b\n",
    "b = json.dumps(a)\n",
    "# 打印b\n",
    "print(b)\n",
    "# 打印b的数据类型\n",
    "print(type(b))\n",
    "# 使用loads()函数，将json格式的字符串b转为列表，赋值给c\n",
    "c = json.loads(b)\n",
    "# 打印c\n",
    "print(c)\n",
    "# 打印c的数据类型\n",
    "print(type(c)) "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1, 2, 3, 4]\n",
      "<class 'str'>\n",
      "[1, 2, 3, 4]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `带参数请求数据`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}